import { useEffect, useState, useRef, useCallback } from 'react';
import SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';
import { useChatStore } from '@/features/chat/store/chatStore';
import { useVoiceStore } from '../store/voiceStore';
import { useGpt } from '../hooks/useGpt';
import { useLanguageStore } from '@/store/languageStore';
import { useParams } from 'react-router-dom';

const apiUrl = import.meta.env.VITE_GPT_API_URL;

const Voice = () => {
  const { listening, transcript, resetTranscript } = useSpeechRecognition();

  const {
    isCovered,
    setIsCovered,
  } = useVoiceStore();

  const [isProcessing, setIsProcessing] = useState(false);
  const [capturedText, setCapturedText] = useState('');

  // ì¹¨ë¬µ ê°ì§€ìš© Ref
  const lastTextTimeRef = useRef<number>(0);

  // ì¤‘ë³µ ì „ì†¡ ë°©ì§€ìš© Ref
  const isSendingRef = useRef(false);

  const { adminId, kioskId } = useParams();
  const { language } = useLanguageStore();
  const langCode = language === 'en' ? 'en-US' : 'ko-KR';

  // DEV ëª¨ë“œ ì…ë ¥ìš© State
  const [devInput, setDevInput] = useState('');

  const addMessage = useChatStore((state) => state.addMessage);
  const updateLastMessage = useChatStore((state) => state.updateLastMessage);
  const setIsCapturing = useChatStore((state) => state.setIsCapturing);
  const isCapturing = useChatStore((state) => state.isCapturing);

  const { sendTextToApi } = useGpt({ apiUrl });

  // ğŸ¤ ë§ˆì´í¬ ë²„íŠ¼ í•¸ë“¤ëŸ¬ (í•«ì›Œë“œ ì—†ì´ ì¦‰ì‹œ ì‹œì‘/ì¤‘ì§€)
  const handleToggleMic = useCallback(async () => {
    try {
      // ì´ë¯¸ ë“£ê³  ìˆê±°ë‚˜ ìº¡ì²˜ ì¤‘ì´ë¼ë©´ ì¤‘ì§€
      if (listening || isCapturing) {
        SpeechRecognition.stopListening();
        setIsCapturing(false);
        setIsProcessing(false);
        return;
      }

      // ì‹œì‘ ë¡œì§
      resetTranscript();      // ê¸°ì¡´ ìë§‰ ì´ˆê¸°í™”
      setIsCapturing(true);   // ìº¡ì²˜ ìƒíƒœ ì‹œì‘
      setIsProcessing(true);  // ì²˜ë¦¬ ì¤‘ ìƒíƒœ
      setCapturedText('');
      lastTextTimeRef.current = Date.now();

      // ë¹ˆ ì‚¬ìš©ì ë§í’ì„  ì¦‰ì‹œ ìƒì„±
      addMessage({
        text: '...',
        isUser: true,
        timestamp: Date.now(),
      });

      // ìŒì„± ì¸ì‹ ì‹œì‘
      await SpeechRecognition.startListening({
        continuous: true,
        language: langCode
      });

    } catch (e) {
      console.error('Mic toggle failed:', e);
    }
  }, [listening, isCapturing, langCode, resetTranscript, setIsCapturing, addMessage]);

  /**
   * âœ… DEV ëª¨ë“œ: í‚¤ë³´ë“œ ì…ë ¥ì„ WebSpeech íë¦„ì²˜ëŸ¼ ì²˜ë¦¬
   */
  const runDevAsIfWebSpeech = useCallback(async (fullText: string) => {
    if (isSendingRef.current) return;
    isSendingRef.current = true;

    const now = Date.now();

    setIsProcessing(true);
    setIsCapturing(true);
    setCapturedText('');
    lastTextTimeRef.current = now;

    addMessage({
      text: '',
      isUser: true,
      timestamp: now,
    });

    updateLastMessage(fullText);
    setCapturedText(fullText);
    lastTextTimeRef.current = Date.now();

    try {
      if (adminId && kioskId) {
        await sendTextToApi(fullText, adminId, kioskId);
      }
    } catch (err) {
      console.error('Error processing DEV input:', err);
    } finally {
      isSendingRef.current = false;
      setIsCapturing(false);
      setIsProcessing(false);
      resetTranscript();
      setCapturedText('');
    }
  }, [addMessage, updateLastMessage, sendTextToApi, adminId, kioskId, resetTranscript, setIsCapturing]);

  // ğŸ“ ì‹¤ì‹œê°„ ìŒì„± ê°ì§€ ë° í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
  useEffect(() => {
    if (transcript && isCapturing) {
      lastTextTimeRef.current = Date.now();
      const currentText = transcript.trim();

      setCapturedText(currentText);
      updateLastMessage(currentText);
    }
  }, [transcript, isCapturing, updateLastMessage]);

  // ğŸ”‡ ë¬´ìŒ ê°ì§€ ë° ìë™ ì „ì†¡
  // ë§í•˜ë‹¤ê°€ 2ì´ˆê°„ ì¹¨ë¬µí•˜ë©´ ìë™ìœ¼ë¡œ ì „ì†¡
  useEffect(() => {
    if (!isCapturing) return;

    const checkInterval = setInterval(() => {
      const now = Date.now();
      // ë§ˆì§€ë§‰ ì…ë ¥ í›„ 2ì´ˆ ê²½ê³¼ ì‹œ ì „ì†¡ ì‹œë„
      if (now - lastTextTimeRef.current > 2000) {
        SpeechRecognition.stopListening(); // ë“£ê¸° ì¤‘ë‹¨
        setIsCapturing(false);
        setIsProcessing(false);

        if (capturedText && adminId && kioskId) {
          sendTextToApi(capturedText, adminId, kioskId).catch((err) => {
            console.error('Error processing voice input:', err);
          });
        } else {
          // ì•„ë¬´ ë§ë„ ì•ˆ í•˜ê³  2ì´ˆ ì§€ë‚˜ë©´ ê·¸ëƒ¥ êº¼ì§
          resetTranscript();
        }

        // ìƒíƒœ ì´ˆê¸°í™”
        resetTranscript();
        setCapturedText('');
      }
    }, 100);

    return () => clearInterval(checkInterval);
  }, [isCapturing, capturedText, sendTextToApi, adminId, kioskId, resetTranscript, setIsCapturing]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ë¦¬ìŠ¤ë‹ ì¤‘ë‹¨
  useEffect(() => {
    return () => {
      SpeechRecognition.stopListening();
    };
  }, []);

  return (
    <div className="p-4 h-fit flex flex-row items-end gap-3 justify-end">

      {/* 1. ë§ˆì´í¬ ë²„íŠ¼ */}
      {!isCovered && (
        <button
          type="button"
          onClick={handleToggleMic}
          className={`
            w-12 h-12 rounded-full shadow-lg flex items-center justify-center transition active:scale-95 flex-shrink-0
            ${listening ? 'bg-red-600 text-white animate-pulse' : 'bg-indigo-600 text-white hover:bg-indigo-700'}
          `}
          title={listening ? 'ë§ˆì´í¬ ë„ê¸°' : 'ë§ˆì´í¬ ì¼œê¸°'}
        >
          {listening ? 'â– ' : 'ğŸ¤'}
        </button>
      )}

      {/* 2. DEV ì „ìš© í‚¤ë³´ë“œ ì…ë ¥ UI */}
      {process.env.NODE_ENV === 'development' && (
        <div className="w-[300px] flex-shrink-0">
          <div className="p-2 rounded-lg border border-indigo-200 bg-white text-left shadow-sm">
            <div className="text-[10px] text-indigo-700 mb-1 font-semibold">
              Developer Input
            </div>
            <div className="flex gap-2">
              <textarea
                className="flex-1 p-2 border rounded-md text-sm resize-none focus:outline-indigo-500 bg-indigo-50"
                rows={1}
                placeholder="ì…ë ¥..."
                value={devInput}
                onChange={(e) => setDevInput(e.target.value)}
                onKeyDown={(e) => {
                  if (e.nativeEvent.isComposing) return;
                  if (e.key === 'Enter' && !e.shiftKey) {
                    e.preventDefault();
                    const text = devInput.trim();
                    if (!text) return;
                    setDevInput('');
                    runDevAsIfWebSpeech(text);
                  }
                }}
              />
              <button
                className="px-3 rounded-md bg-indigo-600 text-white text-sm hover:bg-indigo-700 font-bold whitespace-nowrap"
                disabled={isSendingRef.current}
                onClick={() => {
                  const text = devInput.trim();
                  if (!text) return;
                  setDevInput('');
                  runDevAsIfWebSpeech(text);
                }}
              >
                Send
              </button>
            </div>
          </div>
          <div className="mt-1 text-center">
            {isCapturing ? (
               <span className="text-xs text-indigo-600 animate-pulse font-bold">ì¸ì‹ ì¤‘...</span>
            ) : (
               <span className="text-[10px] text-gray-400">
                 {listening ? 'Listening...' : 'Click Mic to Speak'}
               </span>
            )}
          </div>
        </div>
      )}

      {isCovered && (
        <div
          className="fixed top-0 left-0 w-screen h-screen flex flex-col items-center justify-center bg-white/80 backdrop-blur-md z-50 cursor-pointer"
          onClick={() => {
            setIsCovered(false);
          }}
        >
          <p className="text-4xl font-bold text-indigo-600 animate-pulse">í„°ì¹˜í•˜ì—¬ ì‹œì‘</p>
        </div>
      )}
    </div>
  );
};

export default Voice;